{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\chadchang\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 16537209064671778951\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 9117697966\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 18344537225999046372\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2080 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Dense\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('max_columns', None)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from scipy import stats\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "stop = set(stopwords.words('english'))\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.externals import joblib\n",
    "import json\n",
    "import ast\n",
    "import eli5\n",
    "from functools import reduce\n",
    "import warnings\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "from math import sqrt\n",
    "#from lightgbm import plot_tree\n",
    "from hyperopt import hp, tpe\n",
    "from hyperopt.fmin import fmin\n",
    "from hyperopt import Trials\n",
    "from hyperopt import fmin\n",
    "from hyperopt import STATUS_OK\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "import time\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "#import shap\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend\n",
    "def rmse(y_true, y_pred):\n",
    "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "train = pd.read_csv('FE_train.csv')\n",
    "test  = pd.read_csv('FE_test.csv')\n",
    "data  = pd.concat([train, test], axis=0)\n",
    "\n",
    "\n",
    "#drop outlier\n",
    "train.drop(train[(train['land_area'] > 1500) | (train['building_area'] >1000)].index, inplace= True)\n",
    "\n",
    "\n",
    "train['total_price_log'] = np.log1p(train['total_price'])\n",
    "features = [i for i in train.columns if i not in ['building_id', 'total_price','total_price_log', 'city', 'location_2']] \n",
    "X = train[features].values\n",
    "Y = train['total_price_log'].values\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, kernel_initializer='normal',input_dim = train[features].shape[1], activation='relu'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "model.add(Dense(128, kernel_initializer='normal',activation='relu'))\n",
    "model.add(Dense(128, kernel_initializer='normal',activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, kernel_initializer='normal',activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, kernel_initializer='normal',activation='relu'))\n",
    "\n",
    "\n",
    "# The Output Layer :\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "model.compile(loss=rmse, optimizer='adam', metrics=[rmse])\n",
    "model.fit(X, Y, epochs = 500, batch_size = 5000, validation_split = 0.2, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train = pd.read_csv('dataset-0510/train.csv')\n",
    "#test  = pd.read_csv('dataset-0510/test.csv')\n",
    "\n",
    "train = pd.read_csv('FE_train.csv')\n",
    "test  = pd.read_csv('FE_test.csv')\n",
    "data  = pd.concat([train, test], axis=0)\n",
    "\n",
    "\n",
    "#drop outlier\n",
    "train.drop(train[(train['land_area'] > 1500) | (train['building_area'] >1000)].index, inplace= True)\n",
    "'''\n",
    "target_df = train.groupby(['city', 'town', 'building_type']).agg({'building_area' : ['mean', 'median'], 'land_area' : ['mean', 'median'], 'total_price' : ['mean', 'median']}).reset_index()\n",
    "target_df.columns = [i[0] + '_' + i[1]  if i[1] != '' else i[0] for i in target_df.columns.tolist()]\n",
    "\n",
    "target_df['total_price_median'] = np.log1p(target_df['total_price_median'])\n",
    "target_df['price_land_rate_median'] = target_df['total_price_median'] / target_df['land_area_median']\n",
    "target_df['price_building_rate_median'] = target_df['total_price_median'] / target_df['building_area_median']\n",
    "\n",
    "\n",
    "combine_cols = ['city', 'town', 'building_type', 'price_land_rate_median', 'price_building_rate_median']\n",
    "train = pd.merge(train, target_df[combine_cols], on =['city', 'town', 'building_type'], how='left')\n",
    "test = pd.merge(test, target_df[combine_cols], on =['city', 'town', 'building_type'], how='left')\n",
    "\n",
    "\n",
    "target_df = train.groupby('building_type')['total_price'].median().reset_index()\n",
    "target_df['building_total_price'] = np.log1p(target_df['total_price'])\n",
    "target_df.drop('total_price', axis =1, inplace = True)\n",
    "train = pd.merge(train, target_df, on =['building_type'], how='left')\n",
    "test = pd.merge(test, target_df, on =['building_type'], how='left')\n",
    "\n",
    "train.loc[train['building_area'] == 4, 'parking_area'] = train.loc[train['building_area'] == 4, 'building_area'] / train.loc[train['building_area'] == 4, 'total_floor']\n",
    "test.loc[train['building_area'] == 4, 'parking_area'] = test.loc[test['building_area'] == 4, 'building_area'] / test.loc[test['building_area'] == 4, 'total_floor']\n",
    "'''\n",
    "#drop_cols = [i for i in train.columns if np.sum(train[i]) == 60000 and 'index' in i]\n",
    "train.drop([ 'inter_btw_building_type_building_use',\n",
    " 'inter_btw_building_type_building_material',\n",
    " 'inter_btw_building_type_parking_way',\n",
    " 'inter_btw_building_use_building_material',\n",
    " 'inter_btw_building_use_parking_way',\n",
    " 'inter_btw_building_material_parking_way', 'building_type', 'parking_way','building_material', 'building_use'], axis = 1, inplace = True)\n",
    "test.drop([ 'inter_btw_building_type_building_use',\n",
    " 'inter_btw_building_type_building_material',\n",
    " 'inter_btw_building_type_parking_way',\n",
    " 'inter_btw_building_use_building_material',\n",
    " 'inter_btw_building_use_parking_way',\n",
    " 'inter_btw_building_material_parking_way', 'building_type', 'parking_way','building_material', 'building_use'], axis = 1, inplace = True)\n",
    "#train.drop(drop_cols, axis = 1, inplace = True)\n",
    "#test.drop(drop_cols, axis = 1, inplace = True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 47991 samples, validate on 11998 samples\n",
      "Epoch 1/500\n",
      " - 3s - loss: 12.3307 - rmse: 12.3307 - val_loss: 8.3466 - val_rmse: 8.3466\n",
      "Epoch 2/500\n",
      " - 0s - loss: 5.3546 - rmse: 5.3546 - val_loss: 1.6939 - val_rmse: 1.6939\n",
      "Epoch 3/500\n",
      " - 0s - loss: 1.2709 - rmse: 1.2709 - val_loss: 1.6493 - val_rmse: 1.6493\n",
      "Epoch 4/500\n",
      " - 0s - loss: 1.3879 - rmse: 1.3879 - val_loss: 0.9938 - val_rmse: 0.9938\n",
      "Epoch 5/500\n",
      " - 0s - loss: 0.9720 - rmse: 0.9720 - val_loss: 0.9972 - val_rmse: 0.9972\n",
      "Epoch 6/500\n",
      " - 0s - loss: 0.9731 - rmse: 0.9731 - val_loss: 0.9270 - val_rmse: 0.9270\n",
      "Epoch 7/500\n",
      " - 0s - loss: 0.9389 - rmse: 0.9389 - val_loss: 0.9415 - val_rmse: 0.9415\n",
      "Epoch 8/500\n",
      " - 0s - loss: 0.9260 - rmse: 0.9260 - val_loss: 0.9193 - val_rmse: 0.9193\n",
      "Epoch 9/500\n",
      " - 0s - loss: 0.9265 - rmse: 0.9265 - val_loss: 0.9220 - val_rmse: 0.9220\n",
      "Epoch 10/500\n",
      " - 0s - loss: 0.9220 - rmse: 0.9220 - val_loss: 0.9230 - val_rmse: 0.9230\n",
      "Epoch 11/500\n",
      " - 0s - loss: 0.9212 - rmse: 0.9212 - val_loss: 0.9202 - val_rmse: 0.9202\n",
      "Epoch 12/500\n",
      " - 0s - loss: 0.9220 - rmse: 0.9220 - val_loss: 0.9212 - val_rmse: 0.9212\n",
      "Epoch 13/500\n",
      " - 0s - loss: 0.9214 - rmse: 0.9214 - val_loss: 0.9206 - val_rmse: 0.9206\n",
      "Epoch 14/500\n",
      " - 0s - loss: 0.9199 - rmse: 0.9199 - val_loss: 0.9211 - val_rmse: 0.9211\n",
      "Epoch 15/500\n",
      " - 0s - loss: 0.9194 - rmse: 0.9194 - val_loss: 0.9212 - val_rmse: 0.9212\n",
      "Epoch 16/500\n",
      " - 0s - loss: 0.9190 - rmse: 0.9190 - val_loss: 0.9207 - val_rmse: 0.9207\n",
      "Epoch 17/500\n",
      " - 0s - loss: 0.9191 - rmse: 0.9191 - val_loss: 0.9213 - val_rmse: 0.9213\n",
      "Epoch 18/500\n",
      " - 0s - loss: 0.9194 - rmse: 0.9194 - val_loss: 0.9206 - val_rmse: 0.9206\n",
      "Epoch 19/500\n",
      " - 0s - loss: 0.9186 - rmse: 0.9186 - val_loss: 0.9215 - val_rmse: 0.9215\n",
      "Epoch 20/500\n",
      " - 0s - loss: 0.9186 - rmse: 0.9186 - val_loss: 0.9211 - val_rmse: 0.9211\n",
      "Epoch 21/500\n",
      " - 0s - loss: 0.9175 - rmse: 0.9175 - val_loss: 0.9211 - val_rmse: 0.9211\n",
      "Epoch 22/500\n",
      " - 0s - loss: 0.9169 - rmse: 0.9169 - val_loss: 0.9206 - val_rmse: 0.9206\n",
      "Epoch 23/500\n",
      " - 0s - loss: 0.9168 - rmse: 0.9168 - val_loss: 0.9212 - val_rmse: 0.9212\n",
      "Epoch 24/500\n",
      " - 0s - loss: 0.9169 - rmse: 0.9169 - val_loss: 0.9208 - val_rmse: 0.9208\n",
      "Epoch 25/500\n",
      " - 0s - loss: 0.9172 - rmse: 0.9172 - val_loss: 0.9207 - val_rmse: 0.9207\n",
      "Epoch 26/500\n",
      " - 0s - loss: 0.9160 - rmse: 0.9160 - val_loss: 0.9206 - val_rmse: 0.9206\n",
      "Epoch 27/500\n",
      " - 0s - loss: 0.9167 - rmse: 0.9167 - val_loss: 0.9207 - val_rmse: 0.9207\n",
      "Epoch 28/500\n",
      " - 0s - loss: 0.9162 - rmse: 0.9162 - val_loss: 0.9204 - val_rmse: 0.9204\n",
      "Epoch 29/500\n",
      " - 0s - loss: 0.9161 - rmse: 0.9161 - val_loss: 0.9207 - val_rmse: 0.9207\n",
      "Epoch 30/500\n",
      " - 0s - loss: inf - rmse: inf - val_loss: inf - val_rmse: inf\n",
      "Epoch 31/500\n",
      " - 0s - loss: inf - rmse: inf - val_loss: inf - val_rmse: inf\n",
      "Epoch 32/500\n",
      " - 0s - loss: inf - rmse: inf - val_loss: inf - val_rmse: inf\n",
      "Epoch 33/500\n",
      " - 0s - loss: inf - rmse: inf - val_loss: inf - val_rmse: inf\n",
      "Epoch 34/500\n",
      " - 0s - loss: inf - rmse: inf - val_loss: inf - val_rmse: inf\n",
      "Epoch 35/500\n",
      " - 0s - loss: inf - rmse: inf - val_loss: inf - val_rmse: inf\n",
      "Epoch 36/500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-edf846595d78>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrmse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrmse\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\chadchang\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mc:\\users\\chadchang\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    183\u001b[0m                         \u001b[1;31m# Do not slice the training phase flag.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m                         ins_batch = slice_arrays(\n\u001b[1;32m--> 185\u001b[1;33m                             ins[:-1], batch_ids) + [ins[-1]]\n\u001b[0m\u001b[0;32m    186\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                         \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chadchang\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[1;34m(arrays, start, stop)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chadchang\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    524\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train['total_price_log'] = np.log1p(train['total_price'])\n",
    "features = [i for i in train.columns if i not in ['building_id', 'total_price','total_price_log', 'city', 'location_2']] \n",
    "X = train[features].values\n",
    "Y = train['total_price_log'].values\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, kernel_initializer='uniform',input_dim = train[features].shape[1], activation='sigmoid'))\n",
    "\n",
    "# The Hidden Layers :\n",
    "model.add(Dense(128, kernel_initializer='uniform',activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(500, kernel_initializer='uniform',activation='sigmoid'))\n",
    "\n",
    "# The Output Layer :\n",
    "model.add(Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "model.compile(loss=rmse, optimizer='adam', metrics=[rmse])\n",
    "model.fit(X, Y, epochs = 500, batch_size = 5000, validation_split = 0.2, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        13.381036\n",
       "1        15.015913\n",
       "2        16.074236\n",
       "3        16.469809\n",
       "4        13.544637\n",
       "5        12.890313\n",
       "6        15.956787\n",
       "7        14.333368\n",
       "8        13.802448\n",
       "9        16.284915\n",
       "10       15.849286\n",
       "11       16.235043\n",
       "12       14.987657\n",
       "13       15.385595\n",
       "14       15.829483\n",
       "15       16.168449\n",
       "16       14.861726\n",
       "17       15.236485\n",
       "18       15.690540\n",
       "19       15.616006\n",
       "20       16.425662\n",
       "21       15.496905\n",
       "22       15.471942\n",
       "23       15.331362\n",
       "24       14.836590\n",
       "25       15.537644\n",
       "26       17.785623\n",
       "27       13.905023\n",
       "28       14.891543\n",
       "29       15.616006\n",
       "           ...    \n",
       "59959    13.543883\n",
       "59960    13.940832\n",
       "59961    15.429368\n",
       "59962    15.616009\n",
       "59963    16.380187\n",
       "59964    15.114354\n",
       "59965    16.790066\n",
       "59966    15.719361\n",
       "59967    13.728808\n",
       "59968    15.829555\n",
       "59969    15.480295\n",
       "59970    16.342804\n",
       "59971    14.704439\n",
       "59972    15.631207\n",
       "59973    13.306724\n",
       "59974    16.380188\n",
       "59975    13.836441\n",
       "59976    17.071690\n",
       "59977    14.825907\n",
       "59978    17.442378\n",
       "59979    16.936171\n",
       "59980    15.690540\n",
       "59981    13.728808\n",
       "59982    16.016648\n",
       "59983    15.580458\n",
       "59984    13.802448\n",
       "59985    16.570777\n",
       "59986    16.284915\n",
       "59987    16.710593\n",
       "59988    15.956787\n",
       "Name: total_price_log, Length: 59989, dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['total_price_log']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
