{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.1) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('max_columns', None)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "import datetime\n",
    "import lightgbm as lgb\n",
    "from scipy import stats\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "stop = set(stopwords.words('english'))\n",
    "import os\n",
    "#import xgboost as xgb\n",
    "#import lightgbm as lgb\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.externals import joblib\n",
    "import json\n",
    "import ast\n",
    "import eli5\n",
    "from functools import reduce\n",
    "import warnings\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "from math import sqrt\n",
    "from lightgbm import plot_tree\n",
    "from hyperopt import hp, tpe\n",
    "from hyperopt.fmin import fmin\n",
    "from hyperopt import Trials\n",
    "from hyperopt import fmin\n",
    "from hyperopt import STATUS_OK\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "import time\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "#import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('dataset-0510/train.csv')\n",
    "test  = pd.read_csv('dataset-0510/test.csv')\n",
    "data = pd.concat([train, test], axis=0)\n",
    "#train.drop(['village'], axis =1, inplace = True)\n",
    "#test.drop(['village'], axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Submission(Ids, preds):\n",
    "    file_name = datetime.datetime.today().strftime('%m-%d-%H-%M')\n",
    "    submission = pd.DataFrame({'building_id' : Ids, 'total_price' : preds})\n",
    "    if not os.path.isdir('Submission'):\n",
    "        os.makedirs('Submission')\n",
    "    submission.to_csv('Submission/' + file_name + '.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_importances(feature_importance_df):\n",
    "    cols = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
    "    best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_model(split_num, train, test,features, stratified = False, if_one_hot = True):\n",
    "    '''\n",
    "    category_cols = ['building_material',\n",
    " 'building_type',\n",
    " 'building_use',\n",
    " 'parking_way',\n",
    " 'location_2',\n",
    " 'inter_btw_building_type_parking_way',\n",
    " 'inter_btw_building_type_building_use',\n",
    " 'inter_btw_building_type_building_material',\n",
    " 'inter_btw_parking_way_building_use',\n",
    " 'inter_btw_parking_way_building_material',\n",
    " 'inter_btw_building_use_building_material']\n",
    "    '''\n",
    "    category_cols = ['building_material','building_use','parking_way', 'city']\n",
    "    \n",
    "    if stratified:\n",
    "        kf = StratifiedKFold(n_splits = split_num, random_state = 42, shuffle = True)\n",
    "    else :\n",
    "        kf = KFold(n_splits = split_num, random_state=42, shuffle=True)\n",
    "    train['total_price_log'] = np.log1p(train['total_price'])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    oof = np.zeros(len(train))\n",
    "    predictions = np.zeros(len(test))\n",
    "    \n",
    "    param ={\n",
    "        'n_estimators': 10000, 'max_depth' : -1, 'num_leaves' :30,         \n",
    "        'objective': 'regression',   'metric':'rmse',   \n",
    "        'learning_rate': 0.01,      'boosting': 'gbdt',     'min_data_in_leaf': 10,\n",
    "        'feature_fraction': 0.9,    'bagging_freq':1,       'bagging_fraction': 0.8,     'importance_type': 'gain',\n",
    "        'lambda_l1': 0.2,  'subsample': .8,   'colsample_bytree': .9\n",
    "    }\n",
    "\n",
    "    features = [i for i in train.columns if i not in ['building_id', 'total_price','total_price_log']] \n",
    "     \n",
    "    for fold_, (trn_idx, val_idx) in enumerate(kf.split(train[features].values,train['total_price_log'].values)):\n",
    "        trn_data = lgb.Dataset(train.iloc[trn_idx][features], label= train['total_price_log'].iloc[trn_idx])\n",
    "        val_data = lgb.Dataset(train.iloc[val_idx][features], label= train['total_price_log'].iloc[val_idx])\n",
    "        \n",
    "        \n",
    "        clf = lgb.train(params= param, train_set= trn_data, valid_sets= [trn_data, val_data], verbose_eval=1000, early_stopping_rounds= 10000)\n",
    "        oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration = clf.best_iteration)\n",
    "        predictions += clf.predict(test[features], num_iteration = clf.best_iteration) / kf.n_splits\n",
    "        \n",
    "        y   = np.expm1(train['total_price_log'].iloc[val_idx]) \n",
    "        yhat = np.expm1(oof[val_idx])\n",
    "        Hit_score = np.sum([1 for i in np.abs((y - yhat) / y)  if i <= 0.1 ])\n",
    "        print('fold {} hit_score : {}'.format(fold_ + 1, round(Hit_score, 4) /len(train.iloc[val_idx]) * 10000))\n",
    "        print('-'*30)\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df['feature']    = features\n",
    "        fold_importance_df['importance'] = np.log1p(clf.feature_importance(importance_type='gain', iteration=clf.best_iteration))\n",
    "        fold_importance_df['fold']       = fold_ + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    print('CV scrore : {}'.format(sqrt(mean_squared_error(train['total_price_log'], oof))))\n",
    "    print('-'*30)\n",
    "    y = np.expm1(train['total_price_log']) \n",
    "    yhat = np.expm1(oof)\n",
    "    Hit_score = np.sum([1 for i in np.abs((y - yhat) / y)  if i <= 0.1 ])\n",
    "    print('Hit rate : {}'.format(round(Hit_score, 4) /len(train) * 10000))\n",
    "    \n",
    "    display_importances(feature_importance_df)\n",
    "    return predictions, round(Hit_score, 4) /len(train) * 10000, oof\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10000 rounds.\n",
      "[10000]\ttraining's rmse: 0.0277108\tvalid_1's rmse: 0.177004\n",
      "[20000]\ttraining's rmse: 0.0142966\tvalid_1's rmse: 0.176804\n",
      "Early stopping, best iteration is:\n",
      "[18626]\ttraining's rmse: 0.0152364\tvalid_1's rmse: 0.176792\n",
      "fold 1 hit_score : 5853.889943074004\n",
      "------------------------------\n",
      "Training until validation scores don't improve for 10000 rounds.\n",
      "[10000]\ttraining's rmse: 0.0281996\tvalid_1's rmse: 0.190081\n",
      "Early stopping, best iteration is:\n",
      "[9703]\ttraining's rmse: 0.0290402\tvalid_1's rmse: 0.190076\n",
      "fold 2 hit_score : 5666.824869482677\n",
      "------------------------------\n",
      "Training until validation scores don't improve for 10000 rounds.\n",
      "[10000]\ttraining's rmse: 0.0285357\tvalid_1's rmse: 0.203201\n",
      "[20000]\ttraining's rmse: 0.0148275\tvalid_1's rmse: 0.203202\n",
      "Early stopping, best iteration is:\n",
      "[11634]\ttraining's rmse: 0.0246648\tvalid_1's rmse: 0.203121\n",
      "fold 3 hit_score : 5723.777883246322\n",
      "------------------------------\n",
      "Training until validation scores don't improve for 10000 rounds.\n",
      "[10000]\ttraining's rmse: 0.0284842\tvalid_1's rmse: 0.187846\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-bce1600ae000>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtemp_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'building_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtemp_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'building_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhit_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mif_one_hot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'building_id'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtemp_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'building_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'total_price'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprediction_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mprediction_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-0c8020f53bc1>\u001b[0m in \u001b[0;36mlgb_model\u001b[0;34m(split_num, train, test, features, stratified, if_one_hot)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtrn_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_sets\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrn_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_feature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcategory_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0moof\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/lightgbm/engine.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    214\u001b[0m                                     evaluation_result_list=None))\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mbooster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mevaluation_result_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/lightgbm/basic.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   1758\u001b[0m             _safe_call(_LIB.LGBM_BoosterUpdateOneIter(\n\u001b[1;32m   1759\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1760\u001b[0;31m                 ctypes.byref(is_finished)))\n\u001b[0m\u001b[1;32m   1761\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__is_predicted_cur_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mFalse\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__num_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mis_finished\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "avg_hit_rate = 0\n",
    "prediction_df = pd.DataFrame()\n",
    "hit_score_list = []\n",
    "train_num_list = []\n",
    "building_df_list   = []\n",
    "oof_list       = []\n",
    "features = [i for i in train.columns if i not in ['building_id', 'total_price','total_price_log','city']] \n",
    "for b_type in train['building_type'].unique():\n",
    "    temp_train = train[train['building_type'] == b_type]\n",
    "    temp_test  = test[test['building_type'] == b_type]\n",
    "    temp_train.drop(['building_type'], axis =1, inplace =True)\n",
    "    temp_test.drop(['building_type'], axis =1, inplace =True)\n",
    "    preds, hit_score, oof = lgb_model(5, temp_train, temp_test, features= features,if_one_hot=False)\n",
    "    temp = pd.DataFrame({'building_id' : temp_test['building_id'], 'total_price' : preds})\n",
    "    prediction_df = pd.concat([prediction_df, temp], axis=0)\n",
    "    \n",
    "    print('building_type : {}'.format(b_type))\n",
    "    print('Train_num: {}'.format(len(temp_train)))\n",
    "    print('Test_num: {}'.format(len(temp_test)))\n",
    "    print('-'*1000)\n",
    "    hit_score_list.append(hit_score)\n",
    "    train_num_list.append(len(temp_train))\n",
    "    building_df_list.append(b_type)\n",
    "    oof_list.append(oof)\n",
    "    avg_hit_rate += hit_score / 60000 * len(temp_train)\n",
    "\n",
    "Result_df = pd.DataFrame({'City' : building_df_list,\n",
    "                        'Train_num' : train_num_list,\n",
    "                        'Hit_score' : hit_score_list})\n",
    "\n",
    "print('Avg hit_score : {}'.format(avg_hit_rate))\n",
    "print('總共花：{} 分'.format((time.time() - begin) / 60))\n",
    "print('現在時間 ： {}'.format(datetime.datetime.today().strftime('%m-%d-%H-%M')))\n",
    "Submission(prediction_df['building_id'], np.expm1(prediction_df['total_price']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_train = train[train['building_type'] == 4]\n",
    "temp_test = test[test['building_type'] == 4]\n",
    "\n",
    "temp_train = temp_train.join(pd.get_dummies(temp_train['city'], prefix = 'city'))\n",
    "temp_train = temp_train.join(pd.get_dummies(temp_train['parking_way'], prefix = 'parking_way'))\n",
    "temp_test = temp_test.join(pd.get_dummies(temp_test['city'], prefix = 'city'))\n",
    "temp_test = temp_test.join(pd.get_dummies(temp_test['parking_way'], prefix = 'parking_way'))\n",
    "temp_train['miss_parking_area'] = 0\n",
    "temp_train['miss_parking_price'] = 0\n",
    "temp_train.loc[temp_train['parking_area'].isna(), 'miss_parking_area'] = 1\n",
    "temp_train.loc[temp_train['parking_price'].isna(), 'miss_parking_price'] = 1\n",
    "temp_train['parking_price_every_area'] = temp_train['parking_price'] / temp_train['parking_area']\n",
    "temp_train['parking_way'] = temp_train['parking_way'].astype('category')\n",
    "temp_train = temp_train.join(pd.get_dummies(temp_train['parking_way'], prefix = 'parking_way'))\n",
    "temp_train['parking_area'].fillna(0, inplace =True)\n",
    "temp_train['parking_price'].fillna(0, inplace =True)\n",
    "\n",
    "temp_test['miss_parking_area'] = 0\n",
    "temp_test['miss_parking_price'] = 0\n",
    "temp_test.loc[temp_test['parking_area'].isna(), 'miss_parking_area'] = 1\n",
    "temp_test.loc[temp_test['parking_price'].isna(), 'miss_parking_price'] = 1\n",
    "temp_test['parking_price_every_area'] = temp_test['parking_price'] / temp_test['parking_area']\n",
    "temp_test['parking_way'] = temp_test['parking_way'].astype('category')\n",
    "temp_test = temp_test.join(pd.get_dummies(temp_test['parking_way'], prefix = 'parking_way'))\n",
    "temp_test['parking_area'].fillna(0, inplace =True)\n",
    "temp_test['parking_price'].fillna(0, inplace =True)\n",
    "\n",
    "\n",
    "\n",
    "#temp_train['mean_price'] = temp_train['total_price'] / temp_train['land_area']\n",
    "#temp_test['mean_price'] = 0\n",
    "features = [i for i in temp_train.columns if i not in ['building_id', 'total_price','total_price_log']] \n",
    "#temp_train = temp_train[np.abs(temp_train.land_area-temp_train.land_area.mean()) <= (3*temp_train.land_area.std())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model(5, temp_train, temp_test, features= features,if_one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "city\n",
       "3     7.381115e+06\n",
       "5     3.099744e+06\n",
       "6     5.417316e+06\n",
       "7     2.881146e+07\n",
       "9     9.084332e+06\n",
       "10    7.625755e+06\n",
       "12    1.013288e+07\n",
       "13    1.203804e+08\n",
       "14    6.643299e+06\n",
       "17    4.539870e+06\n",
       "21    8.812216e+06\n",
       "Name: total_price, dtype: float64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.groupby('city')['total_price'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "city\n",
       "3     6.672626e+06\n",
       "5     8.017254e+06\n",
       "6     8.301920e+06\n",
       "7     7.814438e+07\n",
       "9     1.485485e+07\n",
       "10    1.266959e+07\n",
       "12    1.357379e+07\n",
       "13    3.975890e+08\n",
       "14    1.349661e+07\n",
       "17    5.696486e+06\n",
       "21    1.686401e+07\n",
       "Name: total_price, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.groupby('city')['total_price'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
