{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('max_columns', None)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "from scipy import stats\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "stop = set(stopwords.words('english'))\n",
    "import os\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.externals import joblib\n",
    "import json\n",
    "import ast\n",
    "import eli5\n",
    "from functools import reduce\n",
    "import warnings\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "from math import sqrt\n",
    "#from lightgbm import plot_tree\n",
    "from hyperopt import hp, tpe\n",
    "from hyperopt.fmin import fmin\n",
    "from hyperopt import Trials\n",
    "from hyperopt import fmin\n",
    "from hyperopt import STATUS_OK\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "import time\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "#import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2861"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('dataset-0510/train.csv')\n",
    "test  = pd.read_csv('dataset-0510/test.csv')\n",
    "data  = pd.concat([train, test], axis=0)\n",
    "\n",
    "#drop outlier\n",
    "train.drop(train[(train['land_area'] > 1500) | (train['building_area'] >1000)].index, inplace= True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone non-duplicated\n",
    "cols = ['city', 'town','building_type', 'total_floor', 'XIV_MIN', 'building_complete_dt']\n",
    "dup_data = train[train[cols].duplicated(keep=False)]\n",
    "non_dup_data = train[~train[cols].duplicated(keep=False)]\n",
    "#Clone_duplicated = non_dup_data[non_dup_data['building_id'].isin(train['building_id'])]\n",
    "train = pd.concat([train, non_dup_data], axis=0)\n",
    "print(len(train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_encoding(train, validation, test):\n",
    "    \n",
    "    groupby_cols = ['city', 'building_type', 'town']\n",
    "    target_df = train.groupby(groupby_cols).agg({'building_area' : ['mean', 'median'], 'land_area' : ['mean', 'median'], 'total_price' : ['mean', 'median']}).reset_index()\n",
    "    target_df.columns = [i[0] + '_' + i[1]  if i[1] != '' else i[0] for i in target_df.columns.tolist()]\n",
    "    \n",
    "    target_df['price_land_rate_median'] = np.log1p(target_df['total_price_median'] / target_df['land_area_median'])\n",
    "    target_df['price_building_rate_median'] = np.log1p(target_df['total_price_median'] / target_df['building_area_median'])\n",
    "    target_df['price_land_rate_mean'] = np.log1p(target_df['total_price_mean'] / target_df['land_area_mean'])\n",
    "    target_df['price_building_rate_mean'] = np.log1p(target_df['total_price_mean'] / target_df['building_area_mean'])\n",
    "    \n",
    "    \n",
    "    combine_cols = groupby_cols + [ 'price_land_rate_mean', 'price_building_rate_mean', 'price_building_rate_median', 'price_building_rate_median']\n",
    "    train      = pd.merge(train, target_df[combine_cols], on = groupby_cols, how='left')\n",
    "    validation = pd.merge(validation, target_df[combine_cols], on = groupby_cols, how='left')\n",
    "    test       = pd.merge(test, target_df[combine_cols], on = groupby_cols, how='left')\n",
    "    \n",
    "    return train, validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 59989 entries, 0 to 59999\n",
      "Columns: 235 entries, building_id to total_price\n",
      "dtypes: float64(37), int64(197), object(1)\n",
      "memory usage: 108.0+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_score(preds, train_data):\n",
    "    trues  = train_data.get_label()\n",
    "    trues = np.expm1(trues)\n",
    "    preds = np.expm1(preds)\n",
    "    scores = ((np.absolute(preds - trues) / trues) <= 0.1)\n",
    "    hit_score = np.sum(scores) / train_data.num_data()\n",
    "    return 'Hit_score', hit_score, True\n",
    "\n",
    "\n",
    "#categorical feature to one-hot\n",
    "def one_hot(train, test, categorical_features):\n",
    "    data = pd.concat([train, test], axis=0)\n",
    "    for i in categorical_features:\n",
    "        data = data.join(pd.get_dummies(data[i], prefix = i))\n",
    "        data.drop(i, axis = 1, inplace =True)\n",
    "    train = data[:60000]\n",
    "    test  = data[60000:]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_importances(feature_importance_df):\n",
    "    cols = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
    "    best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Submission(Ids, preds):\n",
    "    file_name = datetime.datetime.today().strftime('%m-%d-%H-%M')\n",
    "    submission = pd.DataFrame({'building_id' : Ids, 'total_price' : preds})\n",
    "    if not os.path.isdir('Submission'):\n",
    "        os.makedirs('Submission')\n",
    "    submission.to_csv('Submission/' + file_name + '.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_model(split_num, train, test, stratified = False, if_one_hot = True):\n",
    "    \n",
    "    category_cols = ['city', 'building_type', 'building_material', 'parking_way', 'building_use']\n",
    "\n",
    "    if stratified:\n",
    "        kf = StratifiedKFold(n_splits = split_num, random_state = 42, shuffle = True)\n",
    "    else :\n",
    "        kf = KFold(n_splits = split_num, random_state=42, shuffle=True)\n",
    "    train['price_every_building_area'] = np.log1p(train['total_price'] / train['building_area']) \n",
    "    \n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    oof = np.zeros(len(train))\n",
    "    predictions = np.zeros(len(test))\n",
    "    \n",
    "    param ={\n",
    "        'n_estimators': 10000000, 'max_depth' : -1, 'num_leaves' :30,         \n",
    "        'objective': 'regression', \n",
    "        'learning_rate': 0.01,      'boosting': 'gbdt',     'min_data_in_leaf': 50,\n",
    "        'feature_fraction': 0.9,    'bagging_freq':1,       'bagging_fraction': 0.8,     'importance_type': 'gain',\n",
    "         'subsample': .8,   'colsample_bytree': .9, 'device' : 'cpu', 'num_threads' : 10\n",
    "    }\n",
    "     \n",
    "    features = [i for i in train.columns if i not in ['building_id', 'total_price','price_every_building_area']] \n",
    "    print(len(features))\n",
    "    \n",
    "    for fold_, (trn_idx, val_idx) in enumerate(kf.split(train[features].values,train['price_every_building_area'].values)):\n",
    "        begin = time.time()\n",
    "        folds_train       = train.iloc[trn_idx]\n",
    "        folds_validation  = train.iloc[val_idx]\n",
    "        folds_test        = test\n",
    "\n",
    "        \n",
    "        folds_train, folds_validation, folds_test = target_encoding(folds_train, folds_validation, folds_test)\n",
    "        #features = [i for i in features if i not in ['building_id', 'total_price','total_price_log', 'price_every_building_area','building_type', 'parking_way','building_material', 'building_use'] and 'building_area' not in i] \n",
    "\n",
    "        \n",
    "        trn_data = lgb.Dataset(folds_train[features], label = folds_train['price_every_building_area'])\n",
    "        val_data = lgb.Dataset(folds_validation[features], label = folds_validation['price_every_building_area'])\n",
    "\n",
    "        clf = lgb.train(params = param, train_set = trn_data, valid_sets= [trn_data, val_data], verbose_eval=10000, early_stopping_rounds= 3000, categorical_feature=category_cols, feval = hit_score)\n",
    "        oof[val_idx] = clf.predict(folds_validation[features], num_iteration = clf.best_iteration)\n",
    "        predictions += clf.predict(folds_test[features], num_iteration = clf.best_iteration) / kf.n_splits\n",
    "        \n",
    "        y   = np.expm1(folds_validation['price_every_building_area']) * folds_validation['building_area']\n",
    "        yhat = np.expm1(oof[val_idx]) * folds_validation['building_area']\n",
    "        Hit_score = np.sum([1 for i in np.abs((y - yhat) / y)  if i <= 0.1 ])\n",
    "        print('fold {} hit_score : {}'.format(fold_ + 1, round(Hit_score, 4) /len(train.iloc[val_idx]) * 10000))\n",
    "        print((time.time() - begin) / 60)\n",
    "        print('-'*30)\n",
    "        \n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df['feature']    = features\n",
    "        fold_importance_df['importance'] = np.log1p(clf.feature_importance(importance_type='gain', iteration=clf.best_iteration))\n",
    "        fold_importance_df['fold']       = fold_ + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        \n",
    "    print('CV scrore : {}'.format(sqrt(mean_squared_error(train['price_every_building_area'], oof))))\n",
    "    print('-'*30)\n",
    "    y = np.expm1(train['price_every_building_area'] ) * train['building_area']\n",
    "    yhat = np.expm1(oof) * train['building_area']\n",
    "    Hit_score = np.sum([1 for i in np.abs((y - yhat) / y)  if i <= 0.1 ])\n",
    "    print('Hit ratye : {}'.format(round(Hit_score, 4) /len(train) * 10000))\n",
    "    \n",
    "    #display_importances(feature_importance_df)\n",
    "    return predictions, oof, feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "623\n",
      "Training until validation scores don't improve for 10000 rounds.\n",
      "[10000]\ttraining's l2: 0.0107982\ttraining's Hit_score: 0.722152\tvalid_1's l2: 0.0362552\tvalid_1's Hit_score: 0.538756\n",
      "[20000]\ttraining's l2: 0.00475543\ttraining's Hit_score: 0.87757\tvalid_1's l2: 0.0348985\tvalid_1's Hit_score: 0.565594\n",
      "[30000]\ttraining's l2: 0.00257724\ttraining's Hit_score: 0.951861\tvalid_1's l2: 0.0345081\tvalid_1's Hit_score: 0.583597\n",
      "[40000]\ttraining's l2: 0.0015768\ttraining's Hit_score: 0.980811\tvalid_1's l2: 0.0343597\tvalid_1's Hit_score: 0.589598\n",
      "[50000]\ttraining's l2: 0.00105017\ttraining's Hit_score: 0.990924\tvalid_1's l2: 0.0342952\tvalid_1's Hit_score: 0.591432\n",
      "[60000]\ttraining's l2: 0.000746093\ttraining's Hit_score: 0.995073\tvalid_1's l2: 0.0342694\tvalid_1's Hit_score: 0.593432\n",
      "[70000]\ttraining's l2: 0.000559079\ttraining's Hit_score: 0.997129\tvalid_1's l2: 0.0342499\tvalid_1's Hit_score: 0.5971\n",
      "Early stopping, best iteration is:\n",
      "[69635]\ttraining's l2: 0.000564737\ttraining's Hit_score: 0.996999\tvalid_1's l2: 0.0342497\tvalid_1's Hit_score: 0.597266\n",
      "fold 1 hit_score : 5972.6621103517255\n",
      "81.77165850798289\n",
      "------------------------------\n",
      "Training until validation scores don't improve for 10000 rounds.\n",
      "[10000]\ttraining's l2: 0.01072\ttraining's Hit_score: 0.722875\tvalid_1's l2: 0.0398973\tvalid_1's Hit_score: 0.539423\n",
      "[20000]\ttraining's l2: 0.00470616\ttraining's Hit_score: 0.879181\tvalid_1's l2: 0.0382666\tvalid_1's Hit_score: 0.565261\n",
      "[30000]\ttraining's l2: 0.00254609\ttraining's Hit_score: 0.953454\tvalid_1's l2: 0.0377861\tvalid_1's Hit_score: 0.573929\n",
      "[40000]\ttraining's l2: 0.00155166\ttraining's Hit_score: 0.982145\tvalid_1's l2: 0.0376811\tvalid_1's Hit_score: 0.580097\n",
      "[50000]\ttraining's l2: 0.00103093\ttraining's Hit_score: 0.991869\tvalid_1's l2: 0.0376746\tvalid_1's Hit_score: 0.582264\n",
      "Early stopping, best iteration is:\n",
      "[47164]\ttraining's l2: 0.00114974\ttraining's Hit_score: 0.990072\tvalid_1's l2: 0.0376609\tvalid_1's Hit_score: 0.583431\n",
      "fold 2 hit_score : 5834.3057176196035\n",
      "57.415859059492746\n",
      "------------------------------\n",
      "Training until validation scores don't improve for 10000 rounds.\n",
      "[10000]\ttraining's l2: 0.0107349\ttraining's Hit_score: 0.719763\tvalid_1's l2: 0.0366554\tvalid_1's Hit_score: 0.541757\n",
      "[20000]\ttraining's l2: 0.00471688\ttraining's Hit_score: 0.877644\tvalid_1's l2: 0.0353523\tvalid_1's Hit_score: 0.566594\n",
      "[30000]\ttraining's l2: 0.00255629\ttraining's Hit_score: 0.953158\tvalid_1's l2: 0.0350031\tvalid_1's Hit_score: 0.58093\n",
      "[40000]\ttraining's l2: 0.0015615\ttraining's Hit_score: 0.9817\tvalid_1's l2: 0.0348744\tvalid_1's Hit_score: 0.585431\n",
      "[50000]\ttraining's l2: 0.00103674\ttraining's Hit_score: 0.991517\tvalid_1's l2: 0.0347977\tvalid_1's Hit_score: 0.590765\n",
      "[60000]\ttraining's l2: 0.000734819\ttraining's Hit_score: 0.995462\tvalid_1's l2: 0.0347583\tvalid_1's Hit_score: 0.592265\n",
      "Early stopping, best iteration is:\n",
      "[55343]\ttraining's l2: 0.000855796\ttraining's Hit_score: 0.993999\tvalid_1's l2: 0.0347666\tvalid_1's Hit_score: 0.593266\n",
      "fold 3 hit_score : 5932.655442573762\n",
      "66.58364904324213\n",
      "------------------------------\n",
      "Training until validation scores don't improve for 10000 rounds.\n",
      "[10000]\ttraining's l2: 0.0108026\ttraining's Hit_score: 0.721448\tvalid_1's l2: 0.0366275\tvalid_1's Hit_score: 0.545591\n",
      "[20000]\ttraining's l2: 0.00474496\ttraining's Hit_score: 0.877663\tvalid_1's l2: 0.0351467\tvalid_1's Hit_score: 0.569262\n",
      "[30000]\ttraining's l2: 0.00257369\ttraining's Hit_score: 0.953362\tvalid_1's l2: 0.0347686\tvalid_1's Hit_score: 0.581097\n",
      "[40000]\ttraining's l2: 0.00157783\ttraining's Hit_score: 0.980793\tvalid_1's l2: 0.0346145\tvalid_1's Hit_score: 0.582597\n",
      "[50000]\ttraining's l2: 0.00105168\ttraining's Hit_score: 0.991017\tvalid_1's l2: 0.034555\tvalid_1's Hit_score: 0.585931\n",
      "Early stopping, best iteration is:\n",
      "[47991]\ttraining's l2: 0.00113462\ttraining's Hit_score: 0.989609\tvalid_1's l2: 0.0345636\tvalid_1's Hit_score: 0.588431\n",
      "fold 4 hit_score : 5884.314052342057\n",
      "59.0188603758812\n",
      "------------------------------\n",
      "Training until validation scores don't improve for 10000 rounds.\n",
      "[10000]\ttraining's l2: 0.0107304\ttraining's Hit_score: 0.720837\tvalid_1's l2: 0.0392918\tvalid_1's Hit_score: 0.542424\n",
      "[20000]\ttraining's l2: 0.00473694\ttraining's Hit_score: 0.876903\tvalid_1's l2: 0.0376555\tvalid_1's Hit_score: 0.569262\n",
      "[30000]\ttraining's l2: 0.00257285\ttraining's Hit_score: 0.953214\tvalid_1's l2: 0.0371689\tvalid_1's Hit_score: 0.579263\n",
      "[40000]\ttraining's l2: 0.00157974\ttraining's Hit_score: 0.980756\tvalid_1's l2: 0.0369644\tvalid_1's Hit_score: 0.582764\n",
      "[50000]\ttraining's l2: 0.00105029\ttraining's Hit_score: 0.99135\tvalid_1's l2: 0.0368434\tvalid_1's Hit_score: 0.581597\n",
      "Early stopping, best iteration is:\n",
      "[42519]\ttraining's l2: 0.00141619\ttraining's Hit_score: 0.984367\tvalid_1's l2: 0.036931\tvalid_1's Hit_score: 0.583931\n",
      "fold 5 hit_score : 5839.306551091849\n",
      "53.668438176314034\n",
      "------------------------------\n",
      "Training until validation scores don't improve for 10000 rounds.\n",
      "[10000]\ttraining's l2: 0.0107809\ttraining's Hit_score: 0.721245\tvalid_1's l2: 0.0354704\tvalid_1's Hit_score: 0.538423\n",
      "[20000]\ttraining's l2: 0.00466957\ttraining's Hit_score: 0.879292\tvalid_1's l2: 0.0342559\tvalid_1's Hit_score: 0.565428\n",
      "[30000]\ttraining's l2: 0.00248172\ttraining's Hit_score: 0.954325\tvalid_1's l2: 0.0339666\tvalid_1's Hit_score: 0.576596\n",
      "[40000]\ttraining's l2: 0.00147803\ttraining's Hit_score: 0.981774\tvalid_1's l2: 0.0338765\tvalid_1's Hit_score: 0.581764\n",
      "[50000]\ttraining's l2: 0.000950487\ttraining's Hit_score: 0.992036\tvalid_1's l2: 0.03385\tvalid_1's Hit_score: 0.583597\n",
      "[60000]\ttraining's l2: 0.000648563\ttraining's Hit_score: 0.99561\tvalid_1's l2: 0.0338243\tvalid_1's Hit_score: 0.586598\n",
      "[70000]\ttraining's l2: 0.000464311\ttraining's Hit_score: 0.997166\tvalid_1's l2: 0.0338207\tvalid_1's Hit_score: 0.586431\n",
      "Early stopping, best iteration is:\n",
      "[64680]\ttraining's l2: 0.000551941\ttraining's Hit_score: 0.996425\tvalid_1's l2: 0.0338224\tvalid_1's Hit_score: 0.587598\n",
      "fold 6 hit_score : 5875.979329888314\n",
      "76.88841822544734\n",
      "------------------------------\n",
      "Training until validation scores don't improve for 10000 rounds.\n",
      "[10000]\ttraining's l2: 0.010838\ttraining's Hit_score: 0.7203\tvalid_1's l2: 0.0369954\tvalid_1's Hit_score: 0.536256\n",
      "[20000]\ttraining's l2: 0.00476736\ttraining's Hit_score: 0.875218\tvalid_1's l2: 0.0355108\tvalid_1's Hit_score: 0.564261\n",
      "[30000]\ttraining's l2: 0.00257524\ttraining's Hit_score: 0.951861\tvalid_1's l2: 0.0352412\tvalid_1's Hit_score: 0.578596\n",
      "Early stopping, best iteration is:\n",
      "[29259]\ttraining's l2: 0.0026835\ttraining's Hit_score: 0.948213\tvalid_1's l2: 0.03523\tvalid_1's Hit_score: 0.577263\n",
      "fold 7 hit_score : 5772.62877146191\n",
      "39.69564456542333\n",
      "------------------------------\n",
      "Training until validation scores don't improve for 10000 rounds.\n",
      "[10000]\ttraining's l2: 0.0108487\ttraining's Hit_score: 0.72017\tvalid_1's l2: 0.0382868\tvalid_1's Hit_score: 0.545258\n",
      "[20000]\ttraining's l2: 0.00479236\ttraining's Hit_score: 0.875495\tvalid_1's l2: 0.0367898\tvalid_1's Hit_score: 0.576596\n",
      "[30000]\ttraining's l2: 0.00259294\ttraining's Hit_score: 0.952973\tvalid_1's l2: 0.0363369\tvalid_1's Hit_score: 0.590265\n",
      "[40000]\ttraining's l2: 0.00159084\ttraining's Hit_score: 0.9807\tvalid_1's l2: 0.0361375\tvalid_1's Hit_score: 0.595433\n",
      "[50000]\ttraining's l2: 0.00105924\ttraining's Hit_score: 0.991091\tvalid_1's l2: 0.0360546\tvalid_1's Hit_score: 0.595766\n",
      "Early stopping, best iteration is:\n",
      "[47807]\ttraining's l2: 0.00115108\ttraining's Hit_score: 0.989887\tvalid_1's l2: 0.0360727\tvalid_1's Hit_score: 0.597766\n",
      "fold 8 hit_score : 5977.662943823971\n",
      "58.81939936876297\n",
      "------------------------------\n",
      "Training until validation scores don't improve for 10000 rounds.\n",
      "[10000]\ttraining's l2: 0.0105417\ttraining's Hit_score: 0.723615\tvalid_1's l2: 0.043194\tvalid_1's Hit_score: 0.533422\n",
      "[20000]\ttraining's l2: 0.0045622\ttraining's Hit_score: 0.880311\tvalid_1's l2: 0.041714\tvalid_1's Hit_score: 0.56276\n",
      "[30000]\ttraining's l2: 0.00240595\ttraining's Hit_score: 0.955029\tvalid_1's l2: 0.0412706\tvalid_1's Hit_score: 0.574596\n",
      "[40000]\ttraining's l2: 0.0014337\ttraining's Hit_score: 0.982793\tvalid_1's l2: 0.0411072\tvalid_1's Hit_score: 0.580763\n",
      "[50000]\ttraining's l2: 0.000921933\ttraining's Hit_score: 0.992239\tvalid_1's l2: 0.0410095\tvalid_1's Hit_score: 0.584764\n",
      "[60000]\ttraining's l2: 0.000628136\ttraining's Hit_score: 0.995925\tvalid_1's l2: 0.0409651\tvalid_1's Hit_score: 0.587931\n",
      "[70000]\ttraining's l2: 0.000449582\ttraining's Hit_score: 0.997462\tvalid_1's l2: 0.0409407\tvalid_1's Hit_score: 0.589265\n",
      "Early stopping, best iteration is:\n",
      "[65610]\ttraining's l2: 0.000517171\ttraining's Hit_score: 0.997074\tvalid_1's l2: 0.0409526\tvalid_1's Hit_score: 0.590098\n",
      "fold 9 hit_score : 5900.983497249541\n",
      "77.30241125424703\n",
      "------------------------------\n",
      "Training until validation scores don't improve for 10000 rounds.\n",
      "[10000]\ttraining's l2: 0.0108186\ttraining's Hit_score: 0.721935\tvalid_1's l2: 0.0378577\tvalid_1's Hit_score: 0.536846\n",
      "[20000]\ttraining's l2: 0.00475138\ttraining's Hit_score: 0.878128\tvalid_1's l2: 0.0364501\tvalid_1's Hit_score: 0.565355\n",
      "[30000]\ttraining's l2: 0.00257726\ttraining's Hit_score: 0.953159\tvalid_1's l2: 0.0361277\tvalid_1's Hit_score: 0.581027\n",
      "[40000]\ttraining's l2: 0.00158234\ttraining's Hit_score: 0.981071\tvalid_1's l2: 0.0359693\tvalid_1's Hit_score: 0.583194\n",
      "Early stopping, best iteration is:\n",
      "[35360]\ttraining's l2: 0.00196046\ttraining's Hit_score: 0.971532\tvalid_1's l2: 0.0360299\tvalid_1's Hit_score: 0.584528\n",
      "fold 10 hit_score : 5845.281760586862\n",
      "46.10176317691803\n",
      "------------------------------\n",
      "CV scrore : 0.18981027580258553\n",
      "------------------------------\n",
      "Hit ratye : 5883.578656086949\n",
      "總共花：617.2831571181615 分\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "begin = time.time()\n",
    "prediction, oof, feature_importance_df = lgb_model(10, train, test, stratified = False)\n",
    "prediction = np.expm1(prediction) * test['building_area']\n",
    "print('總共花：{} 分'.format((time.time() - begin) / 60))\n",
    "Submission(test['building_id'], prediction )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1# cv :   0.20573  hit_rate :  5054   public : 5113   note : without Fe set n_estimators = 10,000\n",
    "1.1# cv : 0.20580  hit_rate :  5045   public :        note : without Fe set n_estimators = 10,000 drop village\n",
    "1.2# cv : 0.20352  hit_rate :  5075   public :        note : without Fe set n_estimators = 10,000 drop village one-hot\n",
    "1.3# cv : 0.20570  hit_rate :  5056   public :        note : without Fe set n_estimators = 10,000 drop with category_features\n",
    "\n",
    "# FE and use category_features   CV : 0.20355     hit_rate : 5056  with n_estimators = 10000\n",
    "# FE and use one_hot             CV : 0.20402     hit_rate : 5079  with n_estimators = 10000\n",
    "\n",
    "# FE and use one_hot             CV : 0.20336     hit_rate : 5088  with n_estimators = 10000 fillna mean \n",
    "# FE and use category_features   CV : 0.20320     hit_rate : 5104  with n_estimators = 10000 fillna mean\n",
    "\n",
    "# FE and use one_hot             CV : 0.20342     hit_rate : 5089  with n_estimators = 10000 fillna median \n",
    "# FE and use category_features   CV : 0.2032     hit_rate : 5097  with n_estimators = 10000 fillna median \n",
    "\n",
    "# FE and use one_hot             CV : 0.20342     hit_rate : 5089  with n_estimators = 10000 add min I、II...\n",
    "# FE and use category_features   CV : 0.20362     hit_rate : 5152.0  with n_estimators = 10000  add min I、II...\n",
    "\n",
    "2#    cv : 0.19764  hit_rate :         public : 5737          note : without FE set n_estimators = 1,000,000\n",
    "2.1#  cv : 0.19576  hit_rate : 5634    public : 5803.8749     note : FE set n_estimators = 1,000,000 with one-hot\n",
    "2.2#  cv : 0.19582  hit_rate : 5622    public : 5803.8754     note : FE set n_estimators = 1,000,000 with category_features\n",
    "\n",
    "\n",
    "\n",
    "3# cv : 0.20471  hit_rate :  5063   public :          note :  FE set n_estimators = 10,000  without one-hot\n",
    "4# cv : 0.20536  hit_rate :  5028   public :          note :  FE set n_estimators = 10,000  with one-hot    drop village\n",
    "5# cv : 0.20445  hit_rate :  5081   public :          note :  FE set n_estimators = 10,000  with one-hot   \n",
    "6# cv : 0.20352  hit_rate :  5075   public : 3237     note :  FE set n_estimators = 10,000  with one-hot   combine city、village and groupby encoding\n",
    "\n",
    "7# cv : 0.19362  hit_rate :  5703   public : 5857     note :  just target-encoding and 10-fold\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
