{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "import csv\n",
    "from hyperopt import STATUS_OK\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from hyperopt import tpe\n",
    "from hyperopt import Trials\n",
    "from hyperopt import fmin\n",
    "from hyperopt import hp\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automated_params_tuning(train_dataset, cat_features, max_evals, n_folds, sample_train, sample_test, boost_rounds, early_stop):\n",
    "    # 自動調參 function\n",
    "    # train_dataset: train dataset(panda.Dataframe)\n",
    "    # cat_features: categorial features(list)\n",
    "    # max_evals: evaluation tuning rounds(int)\n",
    "    # n_folds: num of k-fold cv\n",
    "    # sample_train: cv train sample size(int)\n",
    "    # sample_test: cv test sample size(int)\n",
    "    # boost_rounds: cv boost rounds\n",
    "    # early_stop: cv early stop rounds\n",
    "    # return: lightgbm parameters(dict)\n",
    "    \n",
    "    # data preprocessing, log(total_price), drop non-number cols\n",
    "    features = train_dataset.sample(n=sample_train, random_state=31)\n",
    "    features['total_price'] = np.log1p(features['total_price'])\n",
    "    labels = np.array(features['total_price']).reshape((-1, ))\n",
    "    features = features.drop(columns=['total_price', 'building_id'])\n",
    "\n",
    "    # split dataset into train and test used in cv\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = sample_test, random_state = 31)\n",
    "    \n",
    "    train_set = lgb.Dataset(train_features, label = train_labels)\n",
    "    test_set = lgb.Dataset(test_features, label = test_labels)\n",
    "    \n",
    "    # hit score eval function\n",
    "    def hit_score(preds, train_data):\n",
    "        trues  = train_data.get_label()\n",
    "        trues = np.expm1(trues)\n",
    "        preds = np.expm1(preds)\n",
    "        scores = ((np.absolute(preds - trues) / trues) <= 0.1)\n",
    "        hit_score = np.sum(scores) / train_data.num_data()\n",
    "        return 'Hit_score', hit_score, True\n",
    "    \n",
    "    # objective function for optimization\n",
    "    def objective(hyperparameters):\n",
    "        # Keep track of evals\n",
    "        global ITERATION\n",
    "\n",
    "        ITERATION += 1\n",
    "\n",
    "        # Using early stopping to find number of trees trained\n",
    "        if 'n_estimators' in hyperparameters:\n",
    "            del hyperparameters['n_estimators']\n",
    "\n",
    "        # Retrieve the subsample\n",
    "        subsample = hyperparameters['boosting_type'].get('subsample', 1.0)\n",
    "\n",
    "        # Extract the boosting type and subsample to top level keys\n",
    "        hyperparameters['boosting_type'] = hyperparameters['boosting_type']['boosting_type']\n",
    "        hyperparameters['subsample'] = subsample\n",
    "\n",
    "        # Make sure parameters that need to be integers are integers\n",
    "        for parameter_name in ['num_leaves', 'subsample_for_bin', 'min_child_samples']:\n",
    "            hyperparameters[parameter_name] = int(hyperparameters[parameter_name])\n",
    "\n",
    "        start = timer()\n",
    "\n",
    "        # Perform n_folds cross validation\n",
    "        cv_results = lgb.cv(hyperparameters, train_set, num_boost_round = boost_rounds, nfold = n_folds, \n",
    "                            early_stopping_rounds = early_stop, metrics = 'None', seed = 50,\n",
    "                            feval=hit_score, stratified=False, categorical_feature=cat_features)\n",
    "\n",
    "        run_time = timer() - start\n",
    "\n",
    "        # Extract the best score\n",
    "        best_score = cv_results['Hit_score-mean'][-1]\n",
    "\n",
    "        # Loss must be minimized\n",
    "        loss = 1 - best_score\n",
    "\n",
    "        # Boosting rounds that returned the highest cv score\n",
    "        n_estimators = len(cv_results['Hit_score-mean'])\n",
    "\n",
    "        # Add the number of estimators to the hyperparameters\n",
    "        hyperparameters['n_estimators'] = n_estimators\n",
    "\n",
    "        # Write to the csv file ('a' means append)\n",
    "        of_connection = open(OUT_FILE, 'a')\n",
    "        writer = csv.writer(of_connection)\n",
    "        writer.writerow([loss, hyperparameters, ITERATION, run_time, best_score])\n",
    "        of_connection.close()\n",
    "\n",
    "        # Dictionary with information for evaluation\n",
    "        return {'loss': loss, 'hyperparameters': hyperparameters, 'iteration': ITERATION,\n",
    "                'train_time': run_time, 'status': STATUS_OK}\n",
    "\n",
    "    # Define the search space\n",
    "    space = {\n",
    "        'boosting_type': hp.choice('boosting_type', \n",
    "                                    [{'boosting_type': 'gbdt', 'subsample': hp.uniform('gdbt_subsample', 0.5, 1)},]\n",
    "                                  ),\n",
    "        'num_leaves': hp.quniform('num_leaves', 20, 70, 1),\n",
    "        'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.03)),\n",
    "        'subsample_for_bin': hp.quniform('subsample_for_bin', 20000, 300000, 20000),\n",
    "        'min_child_samples': hp.quniform('min_child_samples', 20, 500, 5),\n",
    "        'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n",
    "        'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n",
    "        'colsample_bytree': hp.uniform('colsample_by_tree', 0.6, 1.0),\n",
    "    }\n",
    "    \n",
    "    # Create a new file and open a connection\n",
    "    OUT_FILE = 'bayes_test.csv'\n",
    "    of_connection = open(OUT_FILE, 'w')\n",
    "    writer = csv.writer(of_connection)\n",
    "\n",
    "    # Write column names\n",
    "    headers = ['loss', 'hyperparameters', 'iteration', 'runtime', 'score']\n",
    "    writer.writerow(headers)\n",
    "    of_connection.close()\n",
    "    \n",
    "\n",
    "    # Create the algorithm\n",
    "    tpe_algorithm = tpe.suggest\n",
    "    \n",
    "    # Record results\n",
    "    trials = Trials()\n",
    "    \n",
    "    best = fmin(fn = objective, space = space, algo = tpe.suggest, trials = trials,\n",
    "            max_evals = max_evals)\n",
    "    \n",
    "    trials_dict = sorted(trials.results, key = lambda x: x['loss'])\n",
    "    \n",
    "    return trials_dict[:1][0]['hyperparameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('FE_train_0630.csv')\n",
    "test  = pd.read_csv('FE_test_0630.csv')\n",
    "\n",
    "target_df = train.groupby(['city', 'town']).agg({'building_area' : ['mean', 'median'], 'land_area' : ['mean', 'median'], 'total_price' : ['mean', 'median']}).reset_index()\n",
    "target_df.columns = [i[0] + '_' + i[1]  if i[1] != '' else i[0] for i in target_df.columns.tolist()]\n",
    "target_df['price_land_rate_median'] = np.log1p(target_df['total_price_median']) / target_df['land_area_median']\n",
    "target_df['price_building_rate_median'] = np.log1p(target_df['total_price_median']) / target_df['building_area_median']\n",
    "target_df['price_land_rate_mean'] = np.log1p(target_df['total_price_mean']) / target_df['land_area_mean']\n",
    "target_df['price_building_rate_mean'] = np.log1p(target_df['total_price_mean']) / target_df['building_area_mean']\n",
    "\n",
    "combine_cols = ['city', 'town', 'price_land_rate_median', 'price_building_rate_median', 'price_land_rate_mean', 'price_building_rate_mean']\n",
    "train = pd.merge(train, target_df[combine_cols], on =['city', 'town'], how='left')\n",
    "test = pd.merge(test, target_df[combine_cols], on =['city', 'town'], how='left')\n",
    "\n",
    "train.loc[train['building_area'] == 4, 'parking_area'] = train.loc[train['building_area'] == 4, 'building_area'] / train.loc[train['building_area'] == 4, 'total_floor']\n",
    "test.loc[train['building_area'] == 4, 'parking_area'] = test.loc[test['building_area'] == 4, 'building_area'] / test.loc[test['building_area'] == 4, 'total_floor']\n",
    "drop_cols = [i for i in train.columns if np.sum(train[i]) == 60000 and 'index' in i]\n",
    "\n",
    "train.drop(['town'], axis = 1, inplace = True)\n",
    "test.drop(['town'], axis = 1, inplace = True)\n",
    "train.drop(drop_cols, axis = 1, inplace = True)\n",
    "test.drop(drop_cols, axis = 1, inplace = True)\n",
    "\n",
    "# train.drop(train[(train['land_area'] > 1500) | (train['building_area'] >1000)].index, inplace= True)\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60000, 419), (10000, 419))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [6:52:06<00:00, 205.66s/it, best loss: 0.5351111111111111] \n"
     ]
    }
   ],
   "source": [
    "category = ['building_material', 'building_use', 'building_type', 'city', 'parking_way']\n",
    "\n",
    "ITERATION = 0\n",
    "\n",
    "params = automated_params_tuning(\n",
    "    train, \n",
    "    category,\n",
    "    max_evals=100, \n",
    "    n_folds=5, \n",
    "    sample_train=20000, \n",
    "    sample_test=2000, \n",
    "    boost_rounds=10000, \n",
    "    early_stop=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting_type': 'gbdt',\n",
       " 'colsample_bytree': 0.8340034681089536,\n",
       " 'learning_rate': 0.023294728076622,\n",
       " 'min_child_samples': 30,\n",
       " 'num_leaves': 54,\n",
       " 'reg_alpha': 0.2380166380203602,\n",
       " 'reg_lambda': 0.8778656208518232,\n",
       " 'subsample_for_bin': 200000,\n",
       " 'subsample': 0.8209608370002465,\n",
       " 'n_estimators': 2606}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_default = {\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'regression',\n",
    "        'metric': \"None\",\n",
    "        'num_leaves': 31,\n",
    "        # 'max_bin': 512,\n",
    "        'learning_rate': 0.025,\n",
    "        # 'min_data_in_leaf': 100,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.7,\n",
    "        'bagging_freq': 10,\n",
    "        'verbose': 1,\n",
    "        'num_threads': -1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_auto = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'regression',\n",
    "    'metric': \"None\",\n",
    "    'colsample_bytree': 0.8340034681089536,\n",
    "    'learning_rate': 0.023294728076622,\n",
    "    'min_child_samples': 30,\n",
    "    'num_leaves': 54,\n",
    "    'reg_alpha': 0.2380166380203602,\n",
    "    'reg_lambda': 0.8778656208518232,\n",
    "    'subsample_for_bin': 200000,\n",
    "    'subsample': 0.8209608370002465,\n",
    "#     'n_estimators': 2606,\n",
    "    'n_estimators': 10000,\n",
    "    'verbose': 1,\n",
    "    'num_threads': -1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = ['building_material', 'building_use', 'building_type', 'city', 'parking_way']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train['building_id']\n",
    "train['total_price'] = np.log1p(train['total_price'])\n",
    "\n",
    "y = train['total_price']\n",
    "del train['total_price']\n",
    "X = train.values\n",
    "y = y.values\n",
    "\n",
    "X_train,X_test,y_train,y_test =train_test_split(X, y, test_size=0.1, random_state=31)\n",
    "\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "def hit_score(preds, train_data):\n",
    "    trues  = train_data.get_label()\n",
    "    trues = np.expm1(trues)\n",
    "    preds = np.expm1(preds)\n",
    "    scores = ((np.absolute(preds - trues) / trues) <= 0.1)\n",
    "    hit_score = np.sum(scores) / train_data.num_data()\n",
    "    return 'Hit_score', hit_score, True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\bug87\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\lightgbm\\basic.py:776: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "[1000]\ttraining's Hit_score: 0.495815\tvalid_1's Hit_score: 0.459667\n",
      "[2000]\ttraining's Hit_score: 0.575074\tvalid_1's Hit_score: 0.4875\n",
      "[3000]\ttraining's Hit_score: 0.644315\tvalid_1's Hit_score: 0.501333\n",
      "[4000]\ttraining's Hit_score: 0.703037\tvalid_1's Hit_score: 0.519167\n",
      "[5000]\ttraining's Hit_score: 0.755667\tvalid_1's Hit_score: 0.534167\n",
      "[6000]\ttraining's Hit_score: 0.802407\tvalid_1's Hit_score: 0.543\n",
      "[7000]\ttraining's Hit_score: 0.841037\tvalid_1's Hit_score: 0.552\n",
      "[8000]\ttraining's Hit_score: 0.874426\tvalid_1's Hit_score: 0.553333\n",
      "[9000]\ttraining's Hit_score: 0.901778\tvalid_1's Hit_score: 0.56\n",
      "[10000]\ttraining's Hit_score: 0.924296\tvalid_1's Hit_score: 0.564333\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[9997]\ttraining's Hit_score: 0.924315\tvalid_1's Hit_score: 0.564333\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "feature_name = [i for i in train.columns]\n",
    "\n",
    "model = lgb.train(\n",
    "        params_default, \n",
    "        lgb_train,\n",
    "        num_boost_round=10000, \n",
    "        valid_sets=[lgb_train, lgb_eval], \n",
    "        early_stopping_rounds=1000,\n",
    "        feval=hit_score,\n",
    "        verbose_eval=1000,\n",
    "        categorical_feature=category,\n",
    "        feature_name=feature_name,\n",
    "    )\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE:  0.1324989358706005\n",
      "Hit Rate:  56.43333333333334 %\n",
      "Score:  5644.200834397463\n",
      "Training Time:  370.66744804382324 s\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "y_pred = np.expm1(y_pred)\n",
    "y_test_origin = np.expm1(y_test)\n",
    "\n",
    "hit = np.absolute((y_test_origin - y_pred)/y_test_origin)\n",
    "hit_rate = np.sum(hit < 0.1) / len(hit)\n",
    "MAPE = np.sum(hit)/len(hit)\n",
    "score = hit_rate*(10**4) + (1 - MAPE)\n",
    "\n",
    "print('MAPE: ', MAPE)\n",
    "print('Hit Rate: ', hit_rate * 100,'%')\n",
    "print('Score: ', score)\n",
    "print('Training Time: ', end_time - start_time, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttraining's Hit_score: 0.555352\tvalid_1's Hit_score: 0.482333\n",
      "[2000]\ttraining's Hit_score: 0.66613\tvalid_1's Hit_score: 0.513333\n",
      "[3000]\ttraining's Hit_score: 0.751111\tvalid_1's Hit_score: 0.5325\n",
      "[4000]\ttraining's Hit_score: 0.819741\tvalid_1's Hit_score: 0.543833\n",
      "[5000]\ttraining's Hit_score: 0.87363\tvalid_1's Hit_score: 0.551167\n",
      "[6000]\ttraining's Hit_score: 0.913722\tvalid_1's Hit_score: 0.560833\n",
      "[7000]\ttraining's Hit_score: 0.942759\tvalid_1's Hit_score: 0.563667\n",
      "[8000]\ttraining's Hit_score: 0.962093\tvalid_1's Hit_score: 0.569167\n",
      "[9000]\ttraining's Hit_score: 0.975556\tvalid_1's Hit_score: 0.569667\n",
      "[10000]\ttraining's Hit_score: 0.983037\tvalid_1's Hit_score: 0.570167\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "feature_name = [i for i in train.columns]\n",
    "\n",
    "model = lgb.train(\n",
    "        params_auto, \n",
    "        lgb_train, \n",
    "        valid_sets=[lgb_train, lgb_eval], \n",
    "        feval=hit_score,\n",
    "        verbose_eval=1000,\n",
    "        categorical_feature=category,\n",
    "        feature_name=feature_name,\n",
    "    )\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE:  0.13074515255893515\n",
      "Hit Rate:  57.01666666666667 %\n",
      "Score:  5702.535921514108\n",
      "Training Time:  655.5116527080536 s\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "y_pred = np.expm1(y_pred)\n",
    "y_test_origin = np.expm1(y_test)\n",
    "\n",
    "hit = np.absolute((y_test_origin - y_pred)/y_test_origin)\n",
    "hit_rate = np.sum(hit < 0.1) / len(hit)\n",
    "MAPE = np.sum(hit)/len(hit)\n",
    "score = hit_rate*(10**4) + (1 - MAPE)\n",
    "\n",
    "print('MAPE: ', MAPE)\n",
    "print('Hit Rate: ', hit_rate * 100,'%')\n",
    "print('Score: ', score)\n",
    "print('Training Time: ', end_time - start_time, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
