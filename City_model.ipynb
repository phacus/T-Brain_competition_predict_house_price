{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\chadchang\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "pd.set_option('max_columns', None)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "import datetime\n",
    "import lightgbm as lgb\n",
    "from scipy import stats\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "stop = set(stopwords.words('english'))\n",
    "import os\n",
    "#import xgboost as xgb\n",
    "#import lightgbm as lgb\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.externals import joblib\n",
    "import json\n",
    "import ast\n",
    "import eli5\n",
    "from functools import reduce\n",
    "import warnings\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "from math import sqrt\n",
    "from lightgbm import plot_tree\n",
    "from hyperopt import hp, tpe\n",
    "from hyperopt.fmin import fmin\n",
    "from hyperopt import Trials\n",
    "from hyperopt import fmin\n",
    "from hyperopt import STATUS_OK\n",
    "from hyperopt.pyll.stochastic import sample\n",
    "import time\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "device_lib.list_local_devices()\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('dataset-0510/train.csv')\n",
    "test  = pd.read_csv('dataset-0510/test.csv')\n",
    "data = pd.concat([train, test], axis=0)\n",
    "gc.collect()\n",
    "data.drop([i for i in data.columns if 'index' in i and np.sum(data[i]) == 70000], axis = 1, inplace = True)\n",
    "data.drop(['village', 'doc_rate', 'master_rate', 'bachelor_rate', 'highschool_rate', 'jobschool_rate', 'junior_rate'], axis = 1, inplace = True)\n",
    "train = data[:60000]\n",
    "test = data[60000:]\n",
    "#train.drop(['village'], axis =1, inplace = True)\n",
    "#test.drop(['village'], axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical feature to one-hot\n",
    "def hit_score(preds, train_data):\n",
    "    trues  = train_data.get_label()\n",
    "    trues = np.expm1(trues)\n",
    "    preds = np.expm1(preds)\n",
    "    scores = (np.absolute(preds - trues) / trues) > 0.1\n",
    "    hit_error = np.sum(scores) / train_data.num_data()\n",
    "    return 'hit_error', round(scores), False\n",
    "\n",
    "def one_hot(train, test, categorical_features):\n",
    "    data = pd.concat([train, test], axis=0)\n",
    "    for i in categorical_features:\n",
    "        data = data.join(pd.get_dummies(data[i], prefix = i))\n",
    "        data.drop(i, axis = 1, inplace =True)\n",
    "    train = data[:60000]\n",
    "    test  = data[60000:]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_importances(feature_importance_df):\n",
    "    cols = feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
    "    best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Submission(Ids, preds):\n",
    "    file_name = datetime.datetime.today().strftime('%m-%d-%H-%M')\n",
    "    submission = pd.DataFrame({'building_id' : Ids, 'total_price' : preds})\n",
    "    if not os.path.isdir('Submission'):\n",
    "        os.makedirs('Submission')\n",
    "    submission.to_csv('Submission/' + file_name + '.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_model(split_num, train, test, stratified = False, if_one_hot = True):\n",
    "    \n",
    "\n",
    "    category_cols = ['building_material','building_type','building_use','parking_way']\n",
    "    \n",
    "    if stratified:\n",
    "        kf = StratifiedKFold(n_splits = split_num, random_state = 42, shuffle = True)\n",
    "    else :\n",
    "        kf = KFold(n_splits = split_num, random_state=42, shuffle=True)\n",
    "    train['total_price_log'] = np.log1p(train['total_price'])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    oof = np.zeros(len(train))\n",
    "    predictions = np.zeros(len(test))\n",
    "    \n",
    "    param ={\n",
    "        'n_estimators': 5, 'max_depth' : -1, 'num_leaves' :30,         \n",
    "        'objective': 'regression',   'metric': 'rmse',   \n",
    "        'learning_rate': 0.01,      'boosting': 'gbdt',     'min_data_in_leaf': 10,\n",
    "        'feature_fraction': 0.9,    'bagging_freq':1,       'bagging_fraction': 0.8,     'importance_type': 'gain',\n",
    "        'lambda_l1': 0.2,  'subsample': .8,   'colsample_bytree': .9, 'device' : 'cpu', 'num_thread' : 8\n",
    "    }\n",
    "    features = [i for i in train.columns if i not in ['total_price_log', 'total_price', 'city', 'building_id']]\n",
    "\n",
    "    for fold_, (trn_idx, val_idx) in enumerate(kf.split(train[features].values,train['total_price_log'].values)):\n",
    "        trn_data = lgb.Dataset(train.iloc[trn_idx][features], label= train['total_price_log'].iloc[trn_idx])\n",
    "        val_data = lgb.Dataset(train.iloc[val_idx][features], label= train['total_price_log'].iloc[val_idx])\n",
    "        \n",
    "        clf = lgb.train(params= param, train_set= trn_data, valid_sets= [trn_data, val_data], verbose_eval=100, early_stopping_rounds= 10000, categorical_feature=category_cols, feval=hit_score)\n",
    "        oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration = clf.best_iteration)\n",
    "        predictions += clf.predict(test[features], num_iteration = clf.best_iteration) / kf.n_splits\n",
    "        \n",
    "        y   = np.expm1(train['total_price_log'].iloc[val_idx])\n",
    "        yhat = np.expm1(oof[val_idx])\n",
    "        Hit_score = np.sum([1 for i in np.abs((y - yhat) / y)  if i <= 0.1 ])\n",
    "        print('fold {} hit_score : {}'.format(fold_ + 1, round(Hit_score, 4) /len(train.iloc[val_idx]) * 10000))\n",
    "        print('-'*30)\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df['feature']    = features\n",
    "        fold_importance_df['importance'] = np.log1p(clf.feature_importance(importance_type='gain', iteration=clf.best_iteration))\n",
    "        fold_importance_df['fold']       = fold_ + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    print('CV scrore : {}'.format(sqrt(mean_squared_error(train['total_price_log'], oof))))\n",
    "    print('-'*30)\n",
    "    y = np.expm1(train['total_price_log']) \n",
    "    yhat = np.expm1(oof)\n",
    "    Hit_score = np.sum([1 for i in np.abs((y - yhat) / y)  if i <= 0.1 ])\n",
    "    print('Hit rate : {}'.format(round(Hit_score, 4) /len(train) * 10000))\n",
    "    \n",
    "    display_importances(feature_importance_df)\n",
    "    return predictions, round(Hit_score, 4) /len(train) * 10000, oof\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_process(df):\n",
    "    \n",
    "    #Impute missing value\n",
    "    #df['village_income_median'] = df.groupby(['city', 'town'])['village_income_median'].transform(lambda x : x .fillna(x.median())))\n",
    "    \n",
    "    #floor\n",
    "    df.loc[df['txn_floor'].isna(), 'department'] = 1 \n",
    "    df.loc[df['txn_floor'].notna(), 'department'] = 0\n",
    "    df.loc[df['txn_floor'].isna(), 'txn_floor'] = df.loc[df['txn_floor'].isna(), 'total_floor'] \n",
    "    df['avg_height_floor'] = df['txn_floor'] / df['total_floor'] \n",
    "    df['avg_height_floor'].fillna(0, inplace = True)\n",
    "    \n",
    "    #location\n",
    "    df['location_2'] = df.apply(lambda x : int(str(x['town'])), axis=1)\n",
    "    df['location_2'] = df['location_2'].astype('category')\n",
    "    df['town'] = df['town'].astype('category')\n",
    "    \n",
    "    #date\n",
    "    df['day_between_txn_complete'] = df['txn_dt'] - df['building_complete_dt']\n",
    "    df['year_between_txn_complete'] = round(df['day_between_txn_complete'] / 365)\n",
    "    \n",
    "    #area\n",
    "    #df['land*bulid_area'] = df['land_area'] * df ['building_area']\n",
    "    df['land/bulid_area'] = df['land_area'] / df ['building_area']\n",
    "    df['house_area'] = df['building_area'] - df['parking_area']\n",
    "    df['house_rate']  = df['building_area'] / df['house_area']\n",
    "    \n",
    "    #parking\n",
    "    df['miss_parking_area'] = 0\n",
    "    df['miss_parking_price'] = 0\n",
    "    df.loc[df['parking_area'].isna(), 'miss_parking_area'] = 1\n",
    "    df.loc[df['parking_price'].isna(), 'miss_parking_price'] = 1\n",
    "    df['parking_price_every_area'] = df['parking_price'] / df['parking_area']\n",
    "    df['parking_way'] = df['parking_way'].astype('category')\n",
    "    df['parking_area'].fillna(0, inplace =True)\n",
    "    df['parking_price'].fillna(0, inplace =True)\n",
    "    \n",
    "    #population\n",
    "\n",
    "    \n",
    "    #building   #building_type = 4 is house\n",
    "    df['building_type'] = df['building_type'].astype('category')\n",
    "    df['building_use'] = df['building_use'].astype('category')\n",
    "    df['building_material'] = df['building_material'].astype('category')\n",
    "    #df = df.join(pd.get_dummies(df['building_type'], prefix = 'building_type'))\n",
    "    \n",
    "    #MIN\n",
    "    \n",
    "    MIN_cols = [i for i in df.columns if '_MIN' in i]\n",
    "    df['min_cat'] = 0\n",
    "    for col in [i for i in df.columns if 'MIN' in i]:\n",
    "        df['min_cat'] = df.apply(lambda x : col if x[col] == min(x[MIN_cols]) else x['min_cat'], axis=1)\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df['min_cat'] = le.fit_transform(df['min_cat'])\n",
    "    \n",
    "    \n",
    "    df['MIN_dis_plus'] = reduce(lambda x,y: x + y, [df[i] for i in [i for i in df.columns if '_MIN' in i]])\n",
    "    df['MIN_dis'] = df.apply(lambda x : x[MIN_cols].min(), axis = 1)\n",
    "    df['MAX_dis'] = df.apply(lambda x : x[MIN_cols].max(), axis = 1)\n",
    "    df['Std_dis'] = df.apply(lambda x : x[MIN_cols].std(), axis = 1)\n",
    "    df['Median_dis'] = df.apply(lambda x : x[MIN_cols].median(), axis = 1)\n",
    "    df['Diff_dis'] = df['MAX_dis'] - df['MIN_dis']\n",
    "    \n",
    "    #10 50 100 250 500 1000 5000 10000\n",
    "    df['diff_500_50'] = df['N_500'] - df['N_50']\n",
    "    df['diff_1000_500'] = df['N_1000'] - df['N_500']\n",
    "    df['diff_5000_1000'] = df['N_5000'] - df['N_1000']\n",
    "    df['diff_10000_5000'] = df['N_10000'] - df['N_5000']\n",
    "    df['rate_500_50'] = df['N_500'] / df['N_50']\n",
    "    df['rate_1000_500'] = df['N_1000'] / df['N_500']\n",
    "    df['rate_5000_1000'] = df['N_5000'] / df['N_1000']\n",
    "    df['rate_10000_5000'] = df['N_10000'] / df['N_5000']\n",
    "    \n",
    "    \n",
    "    All_10    = [i for i in df.columns if i.endswith('_10') and 'index' not in i and 'N' not in i]\n",
    "    All_50    = [i for i in df.columns if i.endswith('_50') and 'index' not in i and 'N' not in i]\n",
    "    All_100   = [i for i in df.columns if i.endswith('_100') and 'index' not in i and 'N' not in i]\n",
    "    All_250   = [i for i in df.columns if i.endswith('_250') and 'index' not in i and 'N' not in i]\n",
    "    All_500   = [i for i in df.columns if i.endswith('_500') and 'index' not in i and 'N' not in i]\n",
    "    All_1000  = [i for i in df.columns if i.endswith('_1000') and 'index' not in i and 'N' not in i]\n",
    "    All_5000  = [i for i in df.columns if i.endswith('_5000') and 'index' not in i and 'N' not in i]\n",
    "    All_10000 = [i for i in df.columns if i.endswith('_10000') and 'index' not in i and 'N' not in i]\n",
    "    df['All_10'] = reduce(lambda x, y: x + y, [df[i] for i in All_10])\n",
    "    df['All_50'] = reduce(lambda x, y: x + y, [df[i] for i in All_50])\n",
    "    df['All_100'] = reduce(lambda x, y: x + y, [df[i] for i in All_100])\n",
    "    df['All_250'] = reduce(lambda x, y: x + y, [df[i] for i in All_250])\n",
    "    df['All_500'] = reduce(lambda x, y: x + y, [df[i] for i in All_500])\n",
    "    df['All_1000'] = reduce(lambda x, y: x + y, [df[i] for i in All_1000])\n",
    "    df['All_5000'] = reduce(lambda x, y: x + y, [df[i] for i in All_5000])\n",
    "    df['All_10000'] = reduce(lambda x, y: x + y, [df[i] for i in All_10000])\n",
    "    \n",
    "    '''\n",
    "    for i,j in  zip([All_10, All_50, All_100, All_250, All_500, All_1000, All_5000], [All_50, All_100, All_250, All_500, All_1000, All_5000, All_10000]):\n",
    "        for order in range(len(i)):\n",
    "            df[j[order] + '_' + i[order] + '_rate'] = df[j[order]] / df[i[order]]\n",
    "    '''\n",
    "    #interection\n",
    "    inter_cols = ['building_type', 'parking_way', 'building_use', 'building_material']\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            if j > i:\n",
    "                df['inter_btw_' + inter_cols[i] + '_' + inter_cols[j]] = df.apply(lambda x : str(x[inter_cols[i]]) + str(x[inter_cols[j]]), axis=1)\n",
    "                df['inter_btw_' + inter_cols[i] + '_' + inter_cols[j]] = df['inter_btw_' + inter_cols[i] + '_' + inter_cols[j]].astype('category')\n",
    "    \n",
    "\n",
    "    \n",
    "    #groupby encoding\n",
    "    \n",
    "    category_cols  = ['building_material','building_type','building_use','parking_way','location_2']\n",
    "    numerical_cols = ['building_area', 'land_area', 'day_between_txn_complete', 'txn_dt', 'building_complete_dt', 'house_area', 'house_rate']\n",
    "    statistics = ['mean', 'median', 'max', 'min']\n",
    "    for category in category_cols:\n",
    "        for numerical in numerical_cols:\n",
    "            for stat in statistics:\n",
    "                df[numerical + '_' + stat + '_gb_' + category] = df.groupby([category])[numerical].transform(stat)\n",
    "                df[numerical + '_diff_' + stat + '_gb_' + category] = df[numerical] - df[numerical + '_' + stat + '_gb_' + category]\n",
    "\n",
    "    category_cols =['town', 'location_2']\n",
    "    count_cols = ['building_type', 'building_use', 'building_material', 'parking_way']\n",
    "    for category in category_cols:\n",
    "        for count_col in count_cols:\n",
    "            df['size_gb_' + category + '_' + count_col] = df.groupby([category, count_col])[count_col].transform('size')\n",
    "    \n",
    "    #Polynomail feature\n",
    "    \n",
    "    #useless cols\n",
    "    df.drop([i for i in df.columns if 'index' in i and np.sum(df[i]) == 60000], axis = 1, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.415748504797616"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "begin = time.time()\n",
    "Final_data = feature_process(data)\n",
    "train = Final_data[:60000]\n",
    "test = Final_data[60000:]\n",
    "gc.collect()\n",
    "(time.time() - begin) / 60 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 10000 rounds.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-8323385cc4f2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mtemp_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcombine_cols\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'town'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhit_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moof\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlgb_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mif_one_hot\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'building_id'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mtemp_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'building_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'total_price'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mprediction_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mprediction_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-5a99c7813852>\u001b[0m in \u001b[0;36mlgb_model\u001b[1;34m(split_num, train, test, stratified, if_one_hot)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mval_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'total_price_log'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtrn_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_sets\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtrn_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategorical_feature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcategory_cols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhit_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0moof\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iteration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iteration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mkf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chadchang\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\lightgbm\\engine.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    231\u001b[0m                                         \u001b[0mbegin_iteration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minit_iteration\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m                                         \u001b[0mend_iteration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minit_iteration\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnum_boost_round\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 233\u001b[1;33m                                         evaluation_result_list=evaluation_result_list))\n\u001b[0m\u001b[0;32m    234\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEarlyStopException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mearlyStopException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[0mbooster\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_iteration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mearlyStopException\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_iteration\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chadchang\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\lightgbm\\callback.py\u001b[0m in \u001b[0;36m_callback\u001b[1;34m(env)\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_result_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluation_result_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mbest_score_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mcmp_op\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m                 \u001b[0mbest_score\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m                 \u001b[0mbest_iter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\chadchang\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1476\u001b[0m         raise ValueError(\"The truth value of a {0} is ambiguous. \"\n\u001b[0;32m   1477\u001b[0m                          \u001b[1;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1478\u001b[1;33m                          .format(self.__class__.__name__))\n\u001b[0m\u001b[0;32m   1479\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1480\u001b[0m     \u001b[0m__bool__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "avg_hit_rate = 0\n",
    "prediction_df = pd.DataFrame()\n",
    "hit_score_list = []\n",
    "train_num_list = []\n",
    "city_df_list   = []\n",
    "oof_list       = []\n",
    "\n",
    "for city in train['city'].unique():\n",
    "    temp_train = train[train['city'] == city]\n",
    "    temp_test  = test[test['city'] == city]\n",
    "    temp_train.drop(['city'], axis =1, inplace =True)\n",
    "    temp_test.drop(['city'], axis =1, inplace =True)\n",
    "    \n",
    "    target_df = temp_train.groupby(['town']).agg({'building_area' : ['mean', 'median'], 'land_area' : ['mean', 'median'], 'total_price' : ['mean', 'median']}).reset_index()\n",
    "    target_df.columns = [i[0] + '_' + i[1]  if i[1] != '' else i[0] for i in target_df.columns.tolist()]\n",
    "    target_df['price_land_rate_median'] = target_df['total_price_median'] / target_df['land_area_median']\n",
    "    target_df['price_building_rate_median'] = target_df['total_price_median'] / target_df['building_area_median']\n",
    "    target_df['price_land_rate_mean'] = target_df['total_price_mean'] / target_df['land_area_mean']\n",
    "    target_df['price_building_rate_mean'] = target_df['total_price_mean'] / target_df['building_area_mean']\n",
    "\n",
    "    combine_cols = ['town', 'price_land_rate_median', 'price_building_rate_median', 'price_land_rate_mean', 'price_building_rate_mean']\n",
    "    temp_train = pd.merge(temp_train, target_df[combine_cols], on =['town'], how='left')\n",
    "    temp_test = pd.merge(temp_test, target_df[combine_cols], on =['town'], how='left')\n",
    "    \n",
    "    preds, hit_score, oof = lgb_model(5, temp_train, temp_test, if_one_hot=False)\n",
    "    temp = pd.DataFrame({'building_id' : temp_test['building_id'], 'total_price' : preds})\n",
    "    prediction_df = pd.concat([prediction_df, temp], axis=0)\n",
    "    \n",
    "    print('City : {}'.format(city))\n",
    "    print('Train_num: {}'.format(len(temp_train)))\n",
    "    print('Test_num: {}'.format(len(temp_test)))\n",
    "    print('-'*1000)\n",
    "    hit_score_list.append(hit_score)\n",
    "    train_num_list.append(len(temp_train))\n",
    "    city_df_list.append(city)\n",
    "    oof_list.append(oof)\n",
    "    avg_hit_rate += hit_score / 60000 * len(temp_train)\n",
    "\n",
    "Result_df = pd.DataFrame({'City' : city_df_list,\n",
    "                        'Train_num' : train_num_list,\n",
    "                        'Hit_score' : hit_score_list})\n",
    "\n",
    "print('Avg hit_score : {}'.format(avg_hit_rate))\n",
    "print('總共花：{} 分'.format((time.time() - begin) / 60))\n",
    "print('現在時間 ： {}'.format(datetime.datetime.today().strftime('%m-%d-%H-%M')))\n",
    "Submission(prediction_df['building_id'], np.expm1(prediction_df['total_price']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
